{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yash-1024/ML/blob/main/llama2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WgUeU4FpNRv",
        "outputId": "0082b85c-5818-4d5c-96d5-1b38021414ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Spicyboros-7B-2.2-GPTQ'...\n",
            "remote: Enumerating objects: 39, done.\u001b[K\n",
            "remote: Counting objects: 100% (36/36), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 39 (delta 11), reused 0 (delta 0), pack-reused 3\u001b[K\n",
            "Unpacking objects: 100% (39/39), 498.12 KiB | 2.15 MiB/s, done.\n",
            "Encountered 1 file(s) that may not have been copied correctly on Windows:\n",
            "\tmodel.safetensors\n",
            "\n",
            "See: `git lfs help smudge` for more details.\n"
          ]
        }
      ],
      "source": [
        "! git clone --single-branch --branch gptq-4bit-32g-actorder_True https://huggingface.co/TheBloke/Spicyboros-7B-2.2-GPTQ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMnjc2DUuHhe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac09f65d-814d-4e9e-c60b-c432cd387fb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping transformers as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-e1wc_ew6\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-e1wc_ew6\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit 0a55d9f7376f72ad3ff296d4249840021b03bcc4\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers==4.34.0.dev0)\n",
            "  Downloading huggingface_hub-0.17.1-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.8/294.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.34.0.dev0)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers==4.34.0.dev0)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.34.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.34.0.dev0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.0.dev0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.0.dev0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.0.dev0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.0.dev0) (2023.7.22)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.34.0.dev0-py3-none-any.whl size=7683006 sha256=4ba62c5d2fd621094704f425f267fa39cc76abb0cc06c03f1a37cccefe142fa9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-l4wwqf2u/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\n",
            "Successfully built transformers\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.17.1 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.34.0.dev0\n",
            "Looking in indexes: https://pypi.org/simple, https://huggingface.github.io/autogptq-index/whl/cu118/\n",
            "Collecting auto-gptq\n",
            "  Downloading https://huggingface.github.io/autogptq-index/whl/cu118/auto-gptq/auto_gptq-0.4.2%2Bcu118-cp310-cp310-linux_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate>=0.19.0 (from auto-gptq)\n",
            "  Downloading accelerate-0.23.0-py3-none-any.whl (258 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (2.14.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (1.23.5)\n",
            "Collecting rouge (from auto-gptq)\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (2.0.1+cu118)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.3.3)\n",
            "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (4.34.0.dev0)\n",
            "Collecting peft (from auto-gptq)\n",
            "  Downloading peft-0.5.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.19.0->auto-gptq) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.19.0->auto-gptq) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.19.0->auto-gptq) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.19.0->auto-gptq) (0.17.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->auto-gptq) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->auto-gptq) (16.0.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (4.66.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (3.8.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge->auto-gptq) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2023.3.post1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->auto-gptq) (1.3.0)\n",
            "Installing collected packages: rouge, accelerate, peft, auto-gptq\n",
            "Successfully installed accelerate-0.23.0 auto-gptq-0.4.2+cu118 peft-0.5.0 rouge-1.0.1\n"
          ]
        }
      ],
      "source": [
        "! pip3 uninstall -y transformers\n",
        "! pip3 install git+https://github.com/huggingface/transformers.git\n",
        "! pip3 install transformers>=4.32.0 optimum>=1.12.0\n",
        "! pip3 install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/  # Use cu117 if on CUDA 11.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogygf4JWuOwK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2ef3025-d5a4-4ebc-d8e2-f2e58496999e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.17.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3Ht6t0NndKn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c31b045-c6e9-4625-b8f3-9c6e89581c8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m225.3/232.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdf2image\n",
            "  Downloading pdf2image-1.16.3-py3-none-any.whl (11 kB)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (9.4.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.1)\n",
            "Installing collected packages: pytesseract, PyPDF2, pdf2image\n",
            "Successfully installed PyPDF2-3.0.1 pdf2image-1.16.3 pytesseract-0.3.10\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2 pdf2image pytesseract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDYDkPJJld_C"
      },
      "outputs": [],
      "source": [
        "from PyPDF2 import PdfReader\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "\n",
        "# PDF functions\n",
        "def extract_text_from_text_pdf(pdf_path):\n",
        "    pdf = PdfReader(pdf_path)\n",
        "    text = \"\"\n",
        "    for page_num, page in enumerate(pdf.pages):\n",
        "        text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "def extract_text_from_image_pdf(pdf_path):\n",
        "    pages = convert_from_path(pdf_path)\n",
        "    extracted_text = []\n",
        "    for page_num, page_image in enumerate(pages):\n",
        "        text = pytesseract.image_to_string(page_image)\n",
        "        extracted_text.append(text)\n",
        "    return extracted_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpquEHMsln1w"
      },
      "outputs": [],
      "source": [
        "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# def summarize_text(input_text, max_tokens=600):\n",
        "#     model_name_or_path = \"TheBloke/Spicyboros-7B-2.2-GPTQ\"\n",
        "\n",
        "#     model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
        "#                                                  device_map=\"auto\",\n",
        "#                                                  trust_remote_code=False,\n",
        "#                                                  revision=\"main\")\n",
        "\n",
        "#     tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
        "\n",
        "#     prompt_template = f'''You are a helpful assistant that gives a long summary in layman language and shows output in bulletin format.\n",
        "# USER: Summarise the following text \"{input_text}\"\n",
        "# ASSISTANT:\n",
        "#     '''\n",
        "\n",
        "#     input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
        "#     output = model.generate(inputs=input_ids, max_length=max_tokens, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, repetition_penalty=1.1)\n",
        "\n",
        "#     return tokenizer.decode(output[0])\n",
        "\n",
        "\n",
        "# def main():\n",
        "#     pdf_path = \"/content/website-terms-and-conditions-format.pdf\" # input(\"Enter the path to the PDF file: \")\n",
        "#     try:\n",
        "#         num_pages = 6 #int(input(\"Enter the number of pages in the PDF: \"))\n",
        "#     except ValueError:\n",
        "#         print(\"Invalid input for the number of pages. Please enter a valid integer.\")\n",
        "#         return\n",
        "\n",
        "#     if num_pages <= 0:\n",
        "#         print(\"Number of pages should be greater than zero.\")\n",
        "#         return\n",
        "\n",
        "#     # Determine if the PDF is text-based or image-based\n",
        "#     is_text_pdf = input(\"Is the PDF text-based? (y/n): \").strip().lower()\n",
        "\n",
        "#     if is_text_pdf == 'y':\n",
        "#         text = extract_text_from_text_pdf(pdf_path)\n",
        "#         # Summarize the extracted text using OpenAI\n",
        "#         summary = summarize_text(text)\n",
        "#         print(\"Summary:\")\n",
        "#         print(summary)\n",
        "#     elif is_text_pdf == 'n':\n",
        "#         extracted_text = extract_text_from_image_pdf(pdf_path)\n",
        "#         for page_num, text in enumerate(extracted_text):\n",
        "#             # Summarize the extracted text from each page using OpenAI\n",
        "#             summary = summarize_text(text)\n",
        "#             print(f\"Summary for Page {page_num + 1}:\\n{summary}\")\n",
        "#     else:\n",
        "#         print(\"Invalid input. Please enter 'y' for text-based PDF or 'n' for image-based PDF.\")\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TllRroavHiz"
      },
      "outputs": [],
      "source": [
        "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# def summarize_text(input_text, max_tokens=600):\n",
        "#     model_name_or_path = \"TheBloke/Spicyboros-7B-2.2-GPTQ\"\n",
        "\n",
        "#     model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
        "#                                                  device_map=\"auto\",\n",
        "#                                                  trust_remote_code=False,\n",
        "#                                                  revision=\"main\")\n",
        "\n",
        "#     tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
        "\n",
        "#     prompt_template = f'''You are a helpful assistant that gives a long summary in layman language and shows output in bulletin format.\n",
        "# USER: Summarise the following text \"{input_text}\"\n",
        "# ASSISTANT:\n",
        "#     '''\n",
        "\n",
        "#     input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
        "#     output = model.generate(inputs=input_ids, max_length=max_tokens, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, repetition_penalty=1.1)\n",
        "\n",
        "#     return tokenizer.decode(output[0])\n",
        "\n",
        "# def main():\n",
        "#     # Input text provided by the user\n",
        "#     input_text = \"/content/website-terms-and-conditions-format.pdf\"\n",
        "\n",
        "#     # Summarize the input text using the summarize_text function\n",
        "#     summary = summarize_text(input_text)\n",
        "\n",
        "#     # Print the generated summary\n",
        "#     print(\"Summary:\")\n",
        "#     print(summary)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BdRWi4BPAZ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c8e51e8-acd1-4b0f-d479-7c8335ae9079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            "- <s> You are a helpful assistant that gives a long summary in layman language and shows output in bulletin format.\n",
            "USER: Please provide a clear and straightforward detailed explanation of the text '/content/website-terms-and-conditions-format.pdf' in simple language.\n",
            "Additionally, offer insights from the surrounding text or context that can assist users in understanding how service is meant to be used.\n",
            "Ensure to highlight any potential pitfalls or attempts at deception related to the usage of service to help users avoid any misunderstandings or misuse\n",
            "ASSISTANT:\n",
            "    1. Title: Website Terms and Conditions\n",
            "    2. Author: Company Name\n",
            "    3. Date Created: MM/DD/YYYY\n",
            "    4. Purpose: To establish the terms and conditions under which visitors to the website may use its content and services offered by the author.\n",
            "    5. General Provisions:\n",
            "        a) Visitors must agree with all the terms and conditions before using the website.\n",
            "        b) The company reserves the right to update or change these terms without prior notice.\n",
            "        c) The company does not guarantee the accuracy of information on the website.\n",
            "        d) Copyrighted materials belong solely to the author.\n",
            "        e) Any personal data collected through the site will only be used for internal purposes.\n",
            "    6. Privacy Policy:\n",
            "        a) Personal data collected from visitors will not be disclosed to third parties without consent.\n",
            "        b) Cookies are used to track user behavior but do not contain personal information.\n",
            "        c) Users have the right to withdraw their consent at any time.\n",
            "        d) The company may disclose data if required by law or to protect its rights.\n",
            "        7. Liability Disclaimer:\n",
            "            a) The company shall not be held liable for any damage caused by using the website.\n",
            "            b) The company cannot guarantee uninterrupted access to the website.\n",
            "            c) The company is not responsible for any viruses or malware that might infect your device.\n",
            "            d) The company does not guarantee the security of information transmitted over the internet.\n",
            "            e) The company shall not be held responsible for any damages caused by third party applications integrated into the website.\n",
            "</s>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "def summarize_text(input_text, max_tokens=750, max_part_length=250):\n",
        "    # Set the random seed for both PyTorch and the model\n",
        "    # torch.manual_seed(seed)\n",
        "    model_name_or_path = \"TheBloke/Spicyboros-7B-2.2-GPTQ\"\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
        "                                                 device_map=\"auto\",\n",
        "                                                 trust_remote_code=False,\n",
        "                                                 revision=\"main\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
        "\n",
        "    # Split the input text into multiple parts\n",
        "    parts = [input_text[i:i+max_part_length] for i in range(0, len(input_text), max_part_length)]\n",
        "\n",
        "    # Initialize the summary\n",
        "    full_summary = \"\"\n",
        "\n",
        "    for part in parts:\n",
        "        # Generate a summary for each part\n",
        "        prompt_template = f'''You are a helpful assistant that gives a long summary in layman language and shows output in bulletin format.\n",
        "USER: Please provide a clear and straightforward detailed explanation of the text '{part}' in simple language.\n",
        "Additionally, offer insights from the surrounding text or context that can assist users in understanding how service is meant to be used.\n",
        "Ensure to highlight any potential pitfalls or attempts at deception related to the usage of service to help users avoid any misunderstandings or misuse\n",
        "ASSISTANT:\n",
        "    '''\n",
        "\n",
        "        input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
        "\n",
        "        # Set the random seed for generation\n",
        "        # torch.random.manual_seed(seed)\n",
        "\n",
        "        output = model.generate(inputs=input_ids, max_length=max_tokens, temperature=0.6, do_sample=True, top_p=0.95, top_k=40, repetition_penalty=1.1)\n",
        "\n",
        "        summary = tokenizer.decode(output[0])\n",
        "        full_summary += f\"- {summary}\\n\"\n",
        "\n",
        "    return full_summary\n",
        "\n",
        "def main():\n",
        "    # Input text provided by the user\n",
        "    input_text = \"/content/website-terms-and-conditions-format.pdf\"\n",
        "\n",
        "    # Summarize the input text using the summarize_text function with a specific seed\n",
        "    full_summary = summarize_text(input_text)\n",
        "\n",
        "    # Print the generated summary\n",
        "    print(\"Summary:\")\n",
        "    print(full_summary)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Inb_M_3Uab9B"
      },
      "outputs": [],
      "source": [
        "## Code for OCR\n",
        "# !pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQIU2Xp4T01u"
      },
      "outputs": [],
      "source": [
        "# !pip install pytesseract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAnUVAHcFnsJ"
      },
      "outputs": [],
      "source": [
        "# import pdfplumber\n",
        "\n",
        "# # Initialize an empty string to store the text from all pages\n",
        "# all_text = \"\"\n",
        "\n",
        "# # Open the PDF file\n",
        "# with pdfplumber.open(\"/content/Scan Sep 15, 2023.pdf\") as pdf:\n",
        "#     for page in pdf.pages:\n",
        "#         # Extract text from the current page\n",
        "#         text = page.extract_text()\n",
        "\n",
        "#         # Append the text from the current page to the 'all_text' string\n",
        "#         all_text += text\n",
        "\n",
        "# # Print or use 'all_text' as needed\n",
        "# # print(all_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}